# Modelado de tópicos

En este capítulo nos introduciremos a una técnica de aprendizaje no supervisado en el campo del procesamiento del lenguaje natural: el modelado de tópicos (*topic modeling*). Esta técnica busca construir tópicos o temas en base a las distribuciones de palabras en un conjunto de documentos.

A lo largo de este ejercicio veremos:

1. cómo pre-procesar texto para su posterior análisis;
2. cómo construir vectores de documentos por términos;
3. cómo modelar tópicos;
4. cómo interpretar los tópicos, leyendo los resultados junto con nuestro propio análisis cualitativo, entre otras indagaciones.

Vamos a utilizar esta técnica para intentar explorar **¿Cuál es la tematización del big data en la prensa?**, utilizando un corpus de noticias que incluyen la palabra "big data", tomadas de periódicos digitales argentinos. Nos interesa particularmente indagar de qué manera el big data es contextualizado, de modo tal que el modelado de tópicos pueden asistir al análisis de "frames" discursivos. Este tipo de análisis son útiles para investigar acerca de la construcción social de un fenómeno por parte de un sistema de comunicación, como es la prensa [@Jacobi2016].

## Pre-procesamiento de texto

Como en toda tarea de procesamiento del lenguaje natural, comenzaremos por cargar el corpus y preprocesar el texto.

```{r eval=FALSE, include=FALSE}
noticias_para_anotar <- readRDS("./data_ignore/noticiasLimpias_2026_20191018_130620.rda")
glimpse(noticias_para_anotar)

# elegir 5 fuentes y cortar 20 de cada uno
noticias_para_anotar <- readRDS("./data_ignore/noticiasLimpias_2026_20191018_130620.rda") %>%
  filter(!is.na(fecha)) %>%
  select(id=noticia_id, fecha, titulo=items.title, txt=text, fuente=items.displayLink ) %>% 
  filter(fuente %in% 
           c("www.clarin.com", "www.iprofesional.com", "www.pagina12.com.ar", "tn.com.ar", "www.telam.com.ar"),
         !is.na(txt)) %>%
  group_by(fuente) %>% sample_n(20) %>%
  ungroup() %>%
  saveRDS(file = "data/noticias_curso.rds")

```

```{r echo=TRUE, message=TRUE, warning=FALSE}
library(tidyverse) # para manipular en general
library(tidytext) # para convertir los objetos a formatos requeridos / devueltos por LDA

noticias <- readRDS(file = "data/noticias_curso.rds") %>% mutate(id=1:n())
glimpse(noticias) # miramos la estructura de la base
```

Para poder completar nuestros análisis primeros realizaremos varias tareas de preprocesamiento:

1. Haremos un análisis morfosintático para determinar los distintos componentes de la oración;
2. Reduciremos las palabras a sus *lemmas*, formas básicas de las palabras, sin género ni conjugación;
3. Descartaremos algunas palabras comunes, quedándonos sólo con las más significativas.

Para estas tareas trabajaremos con la librería UdPipe, desarrollada por el [Instituto de linguistica formal y aplicada de la Universidad de la República Checa](https://ufal.mff.cuni.cz/udpipe), que tiene un modelo para procesar texto en castellano. En el capítulo anterior hemos instalado esta librería y descargado el modelo del idioma.

```{r, eval=T, echo=T}
library(udpipe)
modelo_sp <- udpipe_load_model(file = "../dix/spanish-gsd-ud-2.5-191206.udpipe") # ruta al modelo
```

```{r eval=F, echo=T}
noticias_anotadas <- udpipe_annotate( 
  object = modelo_sp, # el modelo de idioma
  x = noticias$txt, # el texto a anotar, 
  doc_id = noticias$id, # el id de cada oracion (el resultado tendrá 1 palabra x fila)
  trace = 20
  ) %>% as.data.frame(.) # convertimos el resultado en data frame
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# guardamos el parseado
# readr::write_csv(x = noticias_anotadas, file = "data_ignore/noticias_anotadas.csv")

# tomamos el parseado
noticias_anotadas <- readr::read_csv(file = "data_ignore/noticias_anotadas.csv")
```

Al igual que en el capítulo anterior, usaremos la información de `upos` para filtrar las palabras que podrían ser más signficativas: adjetivos, verbos, y sustantivos. 
Aquí omitimos los adverbios, ya que no nos interesan las posibles modificaciones del sentido entre palabras cercanas, como negaciones o amplificaciones.
Además introduciremos otro filtro: eliminaremos palabras muy comunes en el lenguaje, que dificilmente puedan ayudarnos a identificar un campo semántico. Para eso recurrimos a un diccionario de palabras comunes, del pack `stopwords`, y eliminaremos esos registros con `filter`. Además, incluimos un conjunto de verbos ad-hoc para ser eliminados.

```{r}
noticias_anotadas2 <- noticias_anotadas %>% 
  filter(upos=="ADJ"| upos=="VERB"| upos=="NOUN") %>% # filtramos por tipo de palabra
  select( doc_id, lemma ) %>% # seleccionamos solo las columnas que nos interesan, esto no es necesario
  filter(!lemma %in% stopwords::stopwords(language = "es")) %>% # filtrar las que no están en la tabla de stopwords
  filter(!lemma %in% c("ser", "decir", "tener", "haber", "estar", "hacer", "ver", "leer")) # filtramos verbos muy comunes
glimpse(noticias_anotadas2)
```

## Vectorizado del texto

Comúnmente, los modelos de *machine learning* son entrenados con datos estructurados en forma de tablas. Cuando trabajamos con texto debemos construir estas tablas a partir de las palabras del documento con el que estemos trabajando. Esto lo hacemos con el *vectorizado*.

Supongamos que tenemos dos documentos con una oración cada uno: `El big data es el conjunto de técnicas que las grandes corporaciones analizan para manipular nuestro pensamiento en función de sus intereses privados` y `Google analiza big data para inferir el ritmo de contagio de la gripe H1N1`, que en su forma lemmatizada y filtrada serían `bigdata ser conjunto tecnica grande corporacion analizar manipular pensamiento funcion interes privado` y `google analizar bigdata inferir ritmo contagio gripe h1n1`.
 
Veamos cómo se vería estas oraciones vectorizadas:

```{r eval=TRUE, echo=FALSE}
data.frame(
        Index = c(1L, 2L),
      bigdata = c(1L, 1L),
          ser = c(1L, 0L),
     conjunto = c(1L, 0L),
      tecnica = c(1L, 0L),
       grande = c(1L, 0L),
  corporacion = c(1L, 0L),
     analizar = c(1L, 1L),
    manipular = c(1L, 0L),
      inferir = c(0L, 1L),
         n... = c(NA, NA)
)
```

Aquí hemos reducido cada oración a una "bolsa de palabras", que ha resignado el contexto de formulación de las expresiones verbales, perdiendo el orden. Nos quedamos entonces sólo con un vocabulario general que, para cada oración, anota la frecuencia de aparición con 1 y 0, es decir, con datos que son interpretables por una computadora y que nos pueden servir para entrenar un modelo de machine learning.

con la función `count()` es muy fácil armar un vector, si usamos como inputs el id del documento y las palabras. Luego, podemos convertir nuestra tabla de distribución de palabras en este tipo de objeto utilizando la función `cast_dtm` de la librería `tidytext`.

```{r}
noticias_dtm <- noticias_anotadas2 %>%
  count(doc_id, lemma, sort = TRUE) %>% # contamos palabras x documento
  cast_dtm(doc_id, lemma, n) # convertimos a vector
noticias_dtm
```

El objeto tipo `DocumentTermMatrix` nos informa la cantidad de documentos y la cantidad de palabras distintas, y nos indica un % de palabras que aparecen 0 veces en un documento (Sparsity). 

## Modelado de tópicos con LDA

> Topic models draw on the notion of distributional semantics (Turney & Pantel, 2010) and particularly make use of the so-called bag of words assumption, i.e., the ordering of words within each document is ignored. To grasp the thematic structure of a document, it is sufficient to describe its distribution of words (Grimmer & Stewart, 2013).
@Maier2018.

> Seemingly unsupervised model becomes extremely supervised due to classification work such as setting number of topics, cleaning data in a particular way with an apriori understanding of "meaningful" clusters and interpreting clusters with parent classes manually
@Bechmann2019


### Sobre el modelo LDA

Para construir los tópicos usaremos el modelo Latent Dirichlet Allocation, a través del pack `topicmodels`. Este modelo genera *tópicos* proponiendo una cierta distribución de todas las palabras del corpus, y calcula la distribución de estos tópicos en cada documentos. 

Lo interesante de esta manera de operativizar los temas, es que cada tópico puede ser entendido como un campo semántico, un conjunto de palabras que suelen correlacionar en distintos documentos. Luego, en el momento del análisis de estos resultados, buscaremos inferir un tema a partir de las palabras que más contribuyen a cada tópico. E.g., podríamos inferir de un tópico en el que contribuyen fuertemente los términos "venta", "producto" y "comprador" al tema "comercio".
Según uno de los autores del modelo, la interpretabilidad de la mayoría de los temas es el resultado de “la estructura estadística del lenguaje y cómo interactúa con los supuestos probabilísticos específicos de LDA” (D. Blei, 2012, p. 79). 

A la vez, las palabras no son exclusivas de un tópico sino que cruzan todos los tópicos con una "contribución" relativa. Esto es justamente lo que nos interesa ya queremos comparar distintas maneras de "contextualizar" al mismo término ("big data") a través de distintos tópicos, caracterizados por el uso de ciertas otras palabras. 

### Aplicar LDA

Vamos a construir el modelo con la función `LDA`. Una decisión importante, que debe ser introducida como un parámetro para realizar los análisis, es el número de tópicos a generar. Empecemos por un número criterioso, rápido para testear, y fácil de examinar, y volvamos sobre este problema.

```{r eval=F, echo=T}
k_topics <- 5 # numero de topicos
noticias_tm <- topicmodels::LDA(noticias_dtm, k = k_topics, method = "Gibbs", control = list(seed = 1:3, nstart=3, verbose=1000))
```

```{r eval=T, echo=F}
# saveRDS(noticias_tm, file = "data_ignore/noticias_tm.rds")
noticias_tm <- readRDS(file = "data_ignore/noticias_tm.rds")
```

```{r}
noticias_tm
```

<!-- 2do: estuve hablando de distribuciones pero serian distribuciones probables -->

Ahora vamos a exportar los resultados en los 2 formatos que nos interesa explorar, utilizando la función `tidy`, y especificando la qué probabilidades que nos interesan:

* **beta**: probabilidad *topico x palabra*;
* **gamma**: probabilidad *topico x documento*;

```{r}
noticias_tm_beta <- tidy(noticias_tm, matrix = "beta")
noticias_tm_gamma <- tidy(noticias_tm, matrix = "gamma")
glimpse(noticias_tm_beta)
glimpse(noticias_tm_gamma)
```

## Interpretar el modelo

Los resultados arrojados por el modelo pueden ser útiles para inferir tópicos. No obstante, esto implica un proceso iterativo de interpretación por parte del investigador, que incluye varios momentos:

1. etiquetado manual y organización de los tópicos;
2. análisis de contenido;
3. validación;

Al igual que en los diseños cualitativos debemos tener en consideración 2 cuestiones: (1) que las distintas tareas y momentos del análisis no son secuenciales sino más bien iterativos, y que constantemente iremos tomando decisiones que afectan (hacia adelante) y que informan (hacia atras) a otros momentos; (2) que todas estas decisiones serán mas claras y robustas si son producto del consenso entre distintos analistas que trabajan en forma autónoma y que documentan e intercambian la razones de sus decisiones [@Auerbach2003]. 

### Etiquetado manual y organización de los tópicos

El etiquetado no es un proceso distinto al de la codificación cualitativa, es decir, a la interpretación interativa de ideas y expresiones repetidas y la imputación de un código o etiqueta que lo identifica. 

En términos de *codeo*, preparar los datos para esta tarea es muy fácil: simplemente listamos los términos que más contribuyen a cada tópico.

```{r fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
noticias_tm_beta %>% # principales términos en cada tópico
  group_by(topic) %>%
  top_n(15) %>%
  ungroup() %>%
  arrange(topic, -beta) %>% # vamos a mostrarlo como grafico
  ggplot(aes(x=reorder(term, (beta)),y=beta)) + 
    geom_col() +
    facet_wrap(~topic, scales = "free_y") +
  coord_flip()
```

El objetivo del análisis que haremos (manualmente) sobre estos datos es el de evaluar si hay un campo coherente de palabras en cada tópico, para luego asignarle una etiqueta que lo describa. En tanto estas son todas inferencias nuestras, en el mejor de los casos, guidados por nuestro conocimiento teórico del fenómeno, nos ubicamos en el plano de las hipótesis. 

Veamos esto con nuestros datos, tendiendo en mente el criterio de construcción del corpus (noticias que incluyen "big data"):

* Tópico 1: digital, país, desarrollo, tecnología, presidente nos permiten asumir que se trata de noticias que vinculan al big data con política tecnológica;
* Tópico 2: dato, bigdata, empresa, sector, herramienta, decisión, nos permite suponer que en las noticias se trate al bigdata como una herramienta para de gestión;
* Tópico 3: campañana, político, público, elección nos permiten inferir el campo de las elecciones políticas;
* Tópico 4: humano, mismo, tiempo, vida, máquina son expresiones esperables en artículos críticos de los avances tecnológicos;
* Tópico 5: información, dato, bigdata, usuario, empresa, internet son nociones que nos remiten a redes sociales y plataformas. Esta es una interpretación (hipotética) de las palabras en relación a los conceptos de la datificación y el big data social [@VanDijck2014].

Vamos a escribir estas etiquetas en un array de nombres de tópicos, por si necesitamos incluirlos en futuros gráficos como etiquetas:

```{r}
topicos_nombres <- c("T1"="1. Pol. tecnológica",
                     "T2"="2. Management",
                     "T3"="3. Elecciones",
                     "T4"="4. Críticas",
                     "T5"="5. Redes sociales")
```

Es importante tener en cuenta que no siempre todos los tópicos presentarán un campo semántico coherente: en muchos casos pueden referir a regularidades propias del tipo de comunicación que estamos analizando (e.g., palabras que remiten a una interacción por parte del usuario, si es que estamos trabajando con contenido tomado de páginas interactivas), o una mixtura de palabras tal que el lugar de permitirnos inferir un campo unívoco, nos resulte incoherente.

Luego, debemos organizar nuestros tópicos:

* **¿Descartamos tópicos irrelevantes?**: Más allá de los tópicos incoherentes o para los que un campo semántico no es tan evidente, podemos decidir filtrar otros tópicos en vistas de su (ir)relevancia para nuestra pregunta teórica. En nuestro caso, todos los tópicos parecen incluir aspectos sociales en los que el big data interviene

* **¿Agrupar tópicos?**: Generalmente, en una codificación cualitativa, el proceso se repite iterativamente, haciendo inferencias cada vez más generales (mayor abstracción) y coordinadas (mayor coherencia), lo que nos permite pasar de los códigos a los temas y argumentos. El modelo LDA no tiene esa estructura jerárquica, pero nosotros podemos agrupar o colapsar tópicos en temáticas más generales. Esto es casi siempre necesario cuando trabajamos con un K elevado.

### Análisis de contenido (muestreo cualitativo)

Dados los objetivos de nuestra investigación -indagar las distintas maneras en que se tematiza al big data en la prensa-, no podemos quedarnos con estos resultados que, en el mejor de los casos, son una de tantas clasificación probables. Más bien, nos interesa hacer un *análisis de contenido* de los documentos en relación a cada tópico [@Krippendorff2004], no sólo para aclarar las etiquetas dadas, sino también para responder a nuestras preguntas de investigación.

Haremos dicho análisis de contenido en dos momentos:[^1] Primero, construyendo una muestra de documentos para analizarlos manualmente; y luego, evaluando la validez de nuestras interpretaciones, explorando los contenidos y metadatos.

[^1]: Nos mantenemos aquí en un diseño *pequeño*: cuando la investigación cuenta con recursos humanos suficientes se pueden plantear otros diseños más complejos, como la codificación manual de un segundo corpus de documentos (nuevos), que se usan para validar el modelo, entre otros experimentos de coherencia semántica (@Maier2018).


El objetivo que perseguismo al hacer un análisis cualitativo en este momento de la exploración es comprender los contextos semánticos en los que se definen las palabras de nuestros tópicos. Para ello, conviene tener preguntas teóricamente guiadas, como por ejemplo: ¿con qué fines se asocia al big data? ¿se lo define explicitamente o se lo da por supuesto? ¿qué actores sociales están involucrados?

En este momento sólo usaremos R para seleccionar documentos de cada tópico (construir una muestra) para su posterior análisis. 
Trabajaremos estas muestras manualmente en otro entorno o software destinado al análisis cualitativo o  [CAQDAS](https://es.wikipedia.org/wiki/Programa_para_el_an%C3%A1lisis_cualitativo_asistido_por_computador). 
(Aunque existe una implementación básica de una interfaz como la de otros CAQDAS en R: RDQA (R package for Qualitative Data Analysis), [https://rqda.r-forge.r-project.org/](https://rqda.r-forge.r-project.org/)).

Una forma muy básica de construir esta muestra puede ser simplemente identificar aquellos documentos que tienen mayor probabilidad en cada tópico, a partir de `noticias_tm_gamma`. Asumamos que con 10 documentos podemos empezar nuestros análisis, aunque recordemos que en *muestreos teóricos* (cualitativos), este número no es fijo sino que se llega por un proceso de exploración:

> El muestreo teórico se realiza para descubrir categorías y sus propiedades, y para sugerir las interrelaciones dentro de una teoría. El muestreo estadístico se realiza para obtener evidencia precisa sobre distribuciones de una población entre categorías, que pueden ser utilizadas en descripciones o verificaciones (Glaser y Strauss, 1967: 62). Por el muestreo teórico el investigador selecciona casos a estudiar según su potencial para ayudar a refinar o expandir los conceptos o teorías ya desarrollados. La «saturación teórica» significa que agregar nuevos casos no representará hallar información adicional por medio de la cual el investigador pueda desarrollar nuevas propiedades de las categorías. @Gialdino2006[^2]

[^2]: Para una discusión de esta idea: @Martinez-Salgado2012

```{r eval=FALSE, include=TRUE}
noticias_tm_gamma %>% 
  group_by(topic) %>%
  slice_max(gamma, n=10)
```

Sin embargo, esto supondría un riesgo. Consideremos la manera en que se distribuye la probabilidad de cada tópico por documento, observando los primeros documentos de nuestro corpus:

```{r}
noticias_tm_gamma %>% filter(document %in% c(1:20)) %>%
  ggplot(aes(x=document,y=gamma,label=topic,color=as.factor(topic))) +
  geom_text(size=5, show.legend = FALSE) +
  coord_flip()
```

En algunos documentos se observa una clara preminencia de un tópico, mientras que en otros esta distribución es más pareja (e.g., documentos #10 y #12). Si nos quedásemos sólo con el tópico que más alto puntúa en cada documento para asignarlos a una muestra, podríamos perder la chance de observar de qué manera algún tópico menos predominante se contextualiza. Para ello conviene construir nuestra muestra para que incluya tanto documentos con los valores más altos de afinidad, y otros seleccionados al azar que incluyan al tópico en un cierto umbral de relevancia. Esta es una decisión que deberemos considerar para otros momentos, como cuando evaluemos la validez.

```{r eval=FALSE, include=TRUE}
muestra_mixta_1 <- noticias_tm_gamma %>% 
  group_by(topic) %>%
  slice_max(gamma, n=5)
muestra_mixta_2 <- noticias_tm_gamma %>% 
  filter(gamma > 0.25) %>%
  filter(!document %in% muestra_mixta_1$document) %>%
  group_by(topic) %>%
  slice_sample(n = 5)
rbind(muestra_mixta_1, muestra_mixta_2)
```

### Validación

En el momento de la validación proponemos hipótesis acerca del contenido de las noticias y su estructura, y las probamos para obtener más información que nos ayude a evaluar las conclusiones llegadas en el análisis cualitativo. 

Podemos pensar en 2 grandes formas de validación:

1. validación de hipótesis con contenidos y metadatos;
2. validación estadística.

El primero consiste en 



```{r}

# noticias %>%
#   group_by(month = lubridate::floor_date(fecha, unit = "month")) %>%
#   summarize(n=n()) %>%
#   ggplot( aes(x = month, y = n, color=topic)) + 
#   geom_line()+
#   scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
#   labs(x = "Publishing date", y = "News articles") + 
#   theme_minimal() +
#   theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
#         panel.background = element_blank() ) +
#   theme(axis.text.x = element_text(angle = 90, hjust = 1)) 


```


DIVIDIR EN; 

2. análisis cualitativo de los documentos;
3. validación estadística y determinación de K;
4. validación de metadatos contra supuestos;
5. explorar relaciones entre tópicos;
6. explorar relaciones y diferencias en los metadatos; 
7. explorar relaciones y diferencias en los contenidos (usando lexicones).

















<!-- # ESTABLEZCO UN CRITERIO DE CORTE PARA LAS MUESTRAS DE DOCUMENTOS POR TOPICOS -->

table(topicos_documentos$topic_label) # distribucion en topicos (rank1)
corte_rank1<-mean(topicos_documentos$topic_prob) # valor medio de rank1
corte_rank2<-mean(topicos_documentos$topic_probdiff_2nd) # valor medio de diferencia con rank2
corte <- mean(topicos_documentos$topic_prob) - mean(topicos_documentos$topic_probdiff_2nd) # valor de corte?

x<-topicos_documentos_tidy %>% filter(score>corte_rank2) %>% group_by(topic) %>% tally(sort = TRUE) # %>% summarise(q=mean(n),s=sd(n))
x$topic
x$n
mean(x$n);sd(x$n);sd(x$n)/mean(x$n)

x$n / nrow(noticias_scrap_y_metadata) * 100
  
  

topicos_documentos_graficos <- topicos_documentos_tidy %>% # una tabla auxiliar para simplificar los graficos
  mutate(topic_name = gsub(pattern = "topic_", replacement = "", topic )) %>%
  mutate(topic_id = as.integer(stringi::stri_match_first_regex(topic_name, "(.*?)\\.")[,1]))

topicos_documentos_graficos %>% # distribucion documentos por topicos con cortes
  ggplot(aes(y=score,x=topic_name)) + 
  geom_boxplot() + 
  geom_hline(color="red", yintercept = corte ) + # media de rank1 - diferencia con rank2
  geom_hline(color="blue", yintercept = corte_rank1 ) + # rank1
  geom_hline(color="blue", yintercept = corte_rank2 ) + # rank2
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))+
  scale_y_continuous("topic score", breaks = seq(0,1,0.1))

<!-- # ARMO UNA TABLA CON TOPICOS, TERMINOS, Y SCORES (RANK1 Y CORTE) -->

rank_corte <- as.data.frame(topicos_nombres) %>% # primero armo una tabla con la cantidad de documentos dentro de ese valor de corte ...
  rename(topic_name=topicos_nombres) %>%
  right_join( topicos_documentos_graficos %>% 
    filter(! topic_id %in% topicos_desechados ) %>%
    group_by(topic_name) %>% filter(score > corte) %>% summarize(corte=n()) ) %>% 
  inner_join( # ... y ahora le sumo el rank1 ...
    as.data.frame(table(topicos_documentos$topic_label)) %>% transform(topic_name=as.character(Var1)) %>% 
      rename(Rank1=Freq) %>% select(topic_name,Rank1) 
  )
terminos_rank <- rank_corte %>% transform(topic=as.integer(gsub("\\..*","",topic_name))) %>% inner_join(topicos_palabra_top) # ... y ahora le sumo las palabras ...
write.csv( terminos_rank, file = paste0("terminos_rank_", topicos_K, "topics", "_",format(Sys.time(), "%Y%m%d_%H%M%S"), ".csv")) # ... y lo exporto

<!-- # EXPORTO UNA TABLA DE LINKS A NOTICIAS POR TOPICO (PARA LOS ANALISIS CUALI ES MAS COMODO LEER DESDE LA FUENTE) -->

noticias_para_anotar <- readRDS("noticiasLimpias_2026_20191003_175315.rda")
topicos_documentos %>% 
  select(doc_id, topic, topic_prob, topic_label) %>%
  left_join( noticias_para_anotar %>% select( doc_id, items.link )) %>%
  write.table(file=paste0("noticias_links_topicos_",k_topics,"_topicos.txt") , row.names = FALSE , quote = FALSE , col.names = TRUE , sep = "|")


### Validación estadística y determinación de K

NO SON CONFIABLES, NO SE CORRESPONDEN CON HUMANOS

5 Discussion
We presented the first validation of the assumed coherence and relevance of topic models using human experiments. For three topic models, we demonstrated that traditional metrics do not capture whether topics are coherent or not. Traditional metrics are, indeed, negatively correlated with the measures of topic quality developed in this paper. Our measures enable new forms of model selection and suggest that practitioners developing topic models should thus focus on evaluations that depend on real-world task performance rather than optimizing likelihood-based measures.

Chang, J., Boyd-Graber, J., Gerris, S., Wang, C., & Blei, D. (2009). Reading Tea Leaves: How Humans Interpret Topic Models. Neural Information Processing Systems.

The quality of the model is assessed by computing its perplexity, i.e. a metric based on the probability of the documents held out for evaluation. Hyperparameter settings then can also be optimized according to highest held out likelihood (Wallach et al., 2009). Although likelihood evaluation is widely used due to its pure automatic nature, Chang et al. (2009) have proven with large user studies that optimal held out likelihood does not correspond to human perception of semantic coherence of topics.




comparing measurements of semantic coherence of topics (see eq. 3.15) as an additional hint to identify overly broad or incoherent topics, and


### Comparar tópicos


<!-- # CALCULO DISTANCIA ENTRE LOS VOCABULARIOS DE CADA TOPICO -->

topicos_palabra_phi <- as.data.frame(t( topicos_palabra_tidy %>% spread(key = topic, value = beta) )) # en lo que sigue preparo phi, para calcular Hellinger
colnames(topicos_palabra_phi) <- as.vector( t( topicos_palabra_phi[1,] ))
topicos_palabra_phi <- topicos_palabra_phi[-1,] 
rownames(topicos_palabra_phi[-topicos_desechados,]) <- topicos_nombres[-topicos_desechados]
topicos_palabra_phi <- as.matrix(topicos_palabra_phi)
topicos_palabra_phi <- type.convert(topicos_palabra_phi)
hc_vocabulario<-hclust(as.dist(textmineR::CalcHellingerDist(topicos_palabra_phi[-topicos_desechados,])), "ward.D")
plot(hc_vocabulario,labels = topicos_nombres[-topicos_desechados], sub = "", main = "" , xlab = "Topics")
rm(topicos_palabra_phi,hc_vocabulario)


<!-- # CORRELACIONES DE TOPICOS (por distribucion basica en documentos) -->

lqs<-setdiff((1:25+5),(topicos_desechados+5))
topic_scores_corr <-  cor( topicos_documentos[lqs] , method = "pearson") # correlacion entre topicos asignados a articulos
ggcorrplot::ggcorrplot(topic_scores_corr, outline.col = "white", show.legend = FALSE,
                       type = "lower", colors = colorRampPalette(c("red","white","green"))(3),
                       lab = TRUE, insig = "blank", method = "square", 
                       hc.order = TRUE , hc.method = "complete")
ggsave(filename = "graficos/correlaciones.jpg",width=15,height = 15,units = "cm",dpi=300)
topic_scores_corr %>% corrr::correlate() %>% corrr::network_plot()
rm(lqs, topic_scores_corr)



### Buscar variaciones en metadatos ()

fecha: hay hechos relevantes?

distribución por source


```{r}
noticias_tm_gamma %>%
  mutate(id=as.integer(document)) %>%
  left_join(noticias %>% select(id,fuente)) %>%
  ggplot(aes(x=fuente,y=gamma)) +
    geom_boxplot() +
    facet_grid(~topic) + 
    coord_flip()
```



### Buscar variaciones en contenidos (lexicones)


<!-- # % articulos que dicen "big data" en los primeros 200 x topico -->

str(topicos_documentos_graficos)
topicos_documentos_graficos %>% # proporcion de articulos que dicen big data en titulo
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score) %>%
  left_join( noticias_para_anotar %>% select( doc_id, keyTitulo ) ) %>%
  group_by(topic_name) %>%
  summarise(bgEnTitulo=sum(keyTitulo)) %>%
  mutate( bgEnTitulo = (bgEnTitulo/200)*100 ) %>%
  ggplot(aes(x=reorder(topic_name,bgEnTitulo),y=bgEnTitulo)) + 
  geom_col() +
  theme_minimal() + 
  coord_flip() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/bg_en_titulo.jpg",width=6,height = 7,units = "cm",dpi=300)

<!-- # q articulos que dicen las 3 v -->

x<-topicos_documentos_graficos %>% # proporcion de articulos que dicen las 3 v
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score) %>%
  left_join( noticias_para_anotar %>% select( doc_id, text ) ) %>%
  mutate( 
    volumen = if_else( grepl("volumen", text, ignore.case = TRUE) ,  1 , 0),
    variedad = if_else( grepl("variedad", text, ignore.case = TRUE), 1 , 0),
    velocidad = if_else( grepl("velocidad", text, ignore.case = TRUE), 1 , 0) ) 
x <- x %>% mutate( vvv = volumen + variedad + velocidad)  
x %>% group_by( topic_name ) %>% summarise(n=sum(vvv)) %>%
  ggplot(aes(x=reorder(topic_name,n),y=n)) + 
  geom_col() +
  theme_minimal() + 
  coord_flip() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/3v.jpg",width=6,height = 7,units = "cm",dpi=300)
rm(x)

<!-- # q articulos que dicen *byte -->

topicos_documentos_graficos %>% # proporcion de articulos que dicen las 3 v
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score) %>%
  left_join( noticias_para_anotar %>% select( doc_id, text ) ) %>%
  mutate( 
    byte = if_else( grepl("byte", text, ignore.case = TRUE) ,  1 , 0)) %>% 
  group_by( topic_name ) %>% summarise(n=sum(byte)) %>%
  ggplot(aes(x=reorder(topic_name,n),y=n)) + 
  geom_col() +
  theme_minimal() + 
  coord_flip() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/byte.jpg",width=6,height = 7,units = "cm",dpi=300)

<!-- # TOPICO 16, 1 Y 17: BIG DATA -->

<!-- # q articulos que dicen platform -->

x<-topicos_documentos_graficos %>% # proporcion de articulos que dicen las 3 v
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score) %>%
  left_join( noticias_para_anotar %>% select( doc_id, text ) ) %>%
  mutate( 
    plataforma = if_else( grepl("plataforma", text, ignore.case = TRUE) ,  1 , 0),
    algoritmo = if_else( grepl("algoritmo", text, ignore.case = TRUE), 1 , 0),
    colec = if_else( grepl("colec", text, ignore.case = TRUE), 1 , 0) ) 
x %>% group_by( topic_name ) %>% 
  #summarise(n=sum(plataforma)) %>%
  summarise(n=sum(algoritmo)) %>%
  #summarise(n=sum(colec)) %>%
  ggplot(aes(x=reorder(topic_name,n),y=n)) + 
  geom_col() +
  theme_minimal() + 
  coord_flip() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/plataforma.jpg",width=6,height = 7,units = "cm",dpi=300)
ggsave(filename = "graficos/algoritmo.jpg",width=6,height = 7,units = "cm",dpi=300)
rm(x)

<!-- # TOPICO 10 y 15 : BIG DATA EN ELECCIONES -->

x<-topicos_documentos_graficos %>% # proporcion de articulos que dicen las 3 v
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score)
x<-x %>% left_join( noticias_para_anotar %>% select( doc_id, text ) ) %>%
  mutate( 
    Macri = if_else( grepl("macri", text, ignore.case = TRUE) ,  1 , 0),
    Kirchner = if_else( grepl("kirchner", text, ignore.case = TRUE), 1 , 0),
    Trump = if_else( grepl("trump", text, ignore.case = TRUE), 1 , 0),
    Clinton = if_else( grepl("clinton", text, ignore.case = TRUE), 1 , 0),
    Nix = if_else( grepl("nix", text, ignore.case = TRUE), 1 , 0),
    Zuckerberg = if_else( grepl("zuckerberg", text, ignore.case = TRUE), 1 , 0)
  )
x <- x %>% group_by( topic_name ) %>% select(-(1:6)) %>% summarise_all(funs(sum)) %>% gather(key="actor",value="n",-topic_name)
x
x %>% ggplot(aes(x=reorder(topic_name,n),y=n,fill=actor)) + 
  geom_col()+
  theme_minimal() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.title = element_blank())+
  coord_flip()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/politicos.jpg",width=12,height = 8,units = "cm",dpi=300)
x %>% ggplot(aes(x=reorder(topic_name,n),y=n,fill=actor)) + 
  geom_bar(position="fill", stat="identity")+
  theme_minimal() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.title = element_blank())+
  coord_flip()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/politicos2.jpg",width=12,height = 8,units = "cm",dpi=300)

<!-- # MECKOCHART DE PERSONALIDADES -->

x2<-x%>%inner_join(x %>% group_by(doc_id,topic_name) %>% select(-(1:6)) %>% summarize(Actores=sum(Macri,Kirchner,Trump,Clinton,Nix,Zuckerberg)))
x2
x2 <- x %>% inner_join(menciones)
x2
menciones<-x %>% group_by(topic_name)%>%summarize(Menciones=sum(Actores))
menciones$xmax<-cumsum(menciones$Menciones)
menciones$xmin<-menciones$xmax - menciones$Menciones
menciones
rm(x2)
rm(menciones)
rm(x)

<!-- # TOPICO 18, 20, 22, 23 : BIG DATA EN BIZ -->

x<-topicos_documentos_graficos %>% # proporcion de articulos que dicen las 3 v
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score) %>%
  left_join( noticias_para_anotar %>% select( doc_id, text ) ) %>%
  mutate( 
    revolucion = if_else( grepl("revolucion", text, ignore.case = TRUE) ,  1 , 0),
    cuarto = if_else( grepl("cuarto", text, ignore.case = TRUE), 1 , 0),
    digital = if_else( grepl("digital", text, ignore.case = TRUE), 1 , 0) ) 

x %>% group_by( topic_name ) %>% summarise(n=sum(revolucion)) %>%
  ggplot(aes(x=reorder(topic_name,n),y=n)) + 
  geom_col() +
  theme_minimal() + 
  coord_flip() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/revolucion.jpg",width=6,height = 7,units = "cm",dpi=300)
rm(x)
