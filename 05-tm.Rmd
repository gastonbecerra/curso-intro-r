# Modelado de tópicos

En este capítulo nos introduciremos a una técnica de aprendizaje no supervisado en el campo del procesamiento del lenguaje natural: el modelado de tópicos (*topic modeling*). Esta técnica busca construir tópicos o temas en base a las distribuciones de palabras en un conjunto de documentos.

A lo largo de este ejercicio veremos:

1. cómo pre-procesar texto para su posterior análisis;
2. cómo construir tablas que crucen documento x términos;
3. cómo modelar tópicos;
4. cómo interpretar los tópicos, leyendo los resultados junto con nuestro propio análisis cualitativo, entre otras indagaciones.

Vamos a utilizar esta técnica para intentar explorar **¿Cuál es la tematización del big data en la prensa?**, utilizando un corpus de noticias que incluyen la palabra "big data", tomadas de periódicos digitales argentinos. Nos interesa particularmente indagar de qué manera el big data es contextualizado, de modo tal que el modelado de tópicos pueden asistir al análisis de "frames" discursivos. Este tipo de análisis son útiles para investigar acerca de la construcción social de un fenómeno por parte de un sistema de comunicación, como es la prensa [@Jacoby2016].

## Pre-procesamiento de texto

Como en toda tarea de procesamiento del lenguaje natural, comenzaremos por cargar el corpus y preprocesar el texto.

```{r eval=FALSE, include=FALSE}
# elegir 5 fuentes y cortar 20 de cada uno
x <- readRDS(file="E:/r/bigdata-medios/data2/noticias_3270_20181017_143053.rda")
readRDS(file="E:/r/bigdata-medios/data2/noticias_3270_20181017_143053.rda") %>%
  select(link=items.link, txt=fullContent, fuente=items.displayLink, titulo=items.title) %>% 
  filter(fuente %in% 
           c("www.clarin.com", "www.iprofesional.com", "www.pagina12.com.ar", "tn.com.ar", "www.telam.com.ar"),
         !is.na(txt)) %>%
  group_by(fuente) %>% sample_n(20) %>%
  ungroup() %>%
  mutate(id=1:n()) %>%
  saveRDS(file = "data/noticias_curso.rds")
```

```{r echo=TRUE, message=TRUE, warning=FALSE}
library(tidyverse) # para manipular en general
library(tidytext) # para convertir los objetos a formatos requeridos / devueltos por LDA

noticias <- readRDS(file = "data/noticias_curso.rds") %>% mutate(id=1:n())
glimpse(noticias) # miramos la estructura de la base
```

Para poder completar nuestros análisis primeros realizaremos varias tareas de preprocesamiento:

1. Haremos un análisis morfosintático para determinar los distintos componentes de la oración;
2. Reduciremos las palabras a sus *lemmas*, formas básicas de las palabras, sin género ni conjugación;
3. Descartaremos algunas palabras comunes, quedándonos sólo con las más significativas.

Para estas tareas trabajaremos con la librería UdPipe, desarrollada por el [Instituto de linguistica formal y aplicada de la Universidad de la República Checa](https://ufal.mff.cuni.cz/udpipe), que tiene un modelo para procesar texto en castellano. En el capítulo anterior hemos instalado esta librería y descargado el modelo del idioma.

```{r, eval=T, echo=T}
library(udpipe)
modelo_sp <- udpipe_load_model(file = "../dix/spanish-gsd-ud-2.5-191206.udpipe") # ruta al modelo
```

```{r eval=F, echo=T}
noticias_anotadas <- udpipe_annotate( 
  object = modelo_sp, # el modelo de idioma
  x = noticias$txt, # el texto a anotar, 
  doc_id = noticias$id, # el id de cada oracion (el resultado tendrá 1 palabra x fila)
  trace = 20
  ) %>% as.data.frame(.) # convertimos el resultado en data frame
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# guardamos el parseado
# readr::write_csv(x = noticias_anotadas, file = "data_ignore/noticias_anotadas.csv")

# tomamos el parseado
noticias_anotadas <- readr::read_csv(file = "data_ignore/noticias_anotadas.csv")
```

Al igual que en el tutorial anterior, usaremos la información de `upos` para filtrar las palabras que podrían ser más signficativas: adjetivos, verbos, y sustantivos. 
Aquí omitimos los adverbios, ya que no nos interesan las posibles modificaciones del sentido entre palabras cercanas, como negaciones o amplificaciones.
Además introduciremos otro filtro: eliminaremos palabras muy comunes en el lenguaje, que dificilmente puedan ayudarnos a identificar un campo semántico. Para eso recurrimos a un diccionario de palabras comunes, del pack `stopwords`, y eliminaremos esos registros con `filter`. Además, incluimos un conjunto de verbos ad-hoc para ser eliminados.

```{r}

noticias_anotadas2 <- noticias_anotadas %>% 
  filter(upos=="ADJ"| upos=="VERB"| upos=="NOUN") %>% # filtramos por tipo de palabra
  select( doc_id, lemma ) %>% # seleccionamos solo las columnas que nos interesan, esto no es necesario
  filter(!lemma %in% stopwords::stopwords(language = "es")) %>% # filtrar las que no están en la tabla de stopwords
  filter(!lemma %in% c("ser", "decir", "tener", "haber", "estar", "hacer", "ver", "leer")) # filtramos verbos muy comunes
glimpse(noticias_anotadas2)
```


## Modelado de tópicos con LDA

### Sobre el modelo LDA

Para construir los tópicos usaremos el modelo Latent Dirichlet Allocation, a través del pack `topicmodels`. Este modelo genera *tópicos* proponiendo una cierta distribución de todas las palabras del corpus, y calcula la distribución de estos tópicos en cada documentos. 

Lo interesante de esta manera de operativizar los temas, es que cada tópico puede ser entendido como un campo semántico, un conjunto de palabras que suelen correlacionar en distintos documentos. Luego, en el momento del análisis de estos resultados, buscaremos inferir un tema a partir de las palabras que más contribuyen a cada tópico. E.g., podríamos inferir de un tópico en el que contribuyen fuertemente los términos "venta", "producto" y "comprador" al tema "comercio".
Según uno de los autores del modelo, la interpretabilidad de la mayoría de los temas es el resultado de “la estructura estadística del lenguaje y cómo interactúa con los supuestos probabilísticos específicos de LDA” (D. Blei, 2012, p. 79). 

A la vez, las palabras no son exclusivas de un tópico sino que cruzan todos los tópicos con una "contribución" relativa. Esto es justamente lo que nos interesa ya queremos comparar distintas maneras de "contextualizar" al mismo término ("big data") a través de distintos tópicos, caracterizados por el uso de ciertas otras palabras. 

### Aplicar LDA

Empezaremos calculando la distribución de palabras de cada documento. 

<!-- 
2do: usar it_dft 
Otro punto de partida podría ser calcular palabras significativas, calculando con un peso más bajo a aquellas palabras que aparecen en más documentos. Usaremos este segundo enfoque más adelante. 
-->

El objeto que necesitamos para construir el modelo es de tipo `DocumentTermMatrix` del pack `tm`, xxxx. 
<!-- 2do: explicar la idea de un vector de palabras -->
Podemos convertir nuestra tabla de distribución de palabras en este tipo de objeto utilizando la función `cast_dtm` de la librería `tidytext`.

```{r}
noticias_dtm <- noticias_anotadas2 %>%
  count(doc_id, lemma, sort = TRUE) %>%
  cast_dtm(doc_id, lemma, n)
noticias_dtm
```

El objeto tipo `DocumentTermMatrix` nos informa la cantidad de documentos y la cantidad de palabras distintas, y nos indica un % de palabras que aparecen 0 veces en un documento (Sparsity). 

Luego, vamos a construir el modelo con la función `LDA`. Una decisión importante, que debe ser introducida como un parámetro para realizar los análisis, es el número de tópicos a generar. Empecemos por un número criterioso, rápido para testear, y fácil de examinar, y volvamos sobre este problema.

```{r eval=F, echo=T}
k_topics <- 6 # numero de topicos
noticias_tm <- topicmodels::LDA(noticias_dtm, k = k_topics, method = "Gibbs", control = list(seed = 1:3, nstart=3, verbose=1000))
```

```{r eval=T, echo=F}
# saveRDS(noticias_tm, file = "data_ignore/noticias_tm.rds")
noticias_tm <- readRDS(file = "data_ignore/noticias_tm.rds")
```

```{r}
noticias_tm
```

<!-- 2do: estuve hablando de distribuciones pero serian distribuciones probables -->

Ahora vamos a exportar los resultados en los 2 formatos que nos interesa explorar, utilizando la función `tidy`, y especificando la qué probabilidades que nos interesan:

* **beta**: probabilidad *topico x palabra*;
* **gamma**: probabilidad *topico x documento*;

```{r}
noticias_tm_beta <- tidy(noticias_tm, matrix = "beta")
noticias_tm_gamma <- tidy(noticias_tm, matrix = "gamma")
glimpse(noticias_tm_beta)
glimpse(noticias_tm_gamma)
```

## Interpretar el modelo

Los resultados arrojados por el modelo pueden ser útiles para inferir tópicos. No obstante, esto implica un proceso iterativo de interpretación por parte del investigador, que incluye varios momentos:

1. etiquetado manual y organización de los tópicos;
2. validación cualitativa y estadística;
3. heurística.

Este proceso de interpretación tiene 

DESAFIOS DE INV CUALI

ABRIR CAJA NEGRA
Especialmente, el desafío de "transparentar" la parte humana de este análisis para asegurar la credibilidad de nuestras inferencias y decisiones, a la vez que permitir interpretaciones alternativas. 

Al igual que en los diseños cualitativos debemos tener en consideración 2 cuestiones: (1) que las distintas tareas y momentos del análisis no son secuenciales sino más bien iterativos, y que constantemente iremos tomando decisiones que afectan (hacia adelante) y que informan (hacia atras) a otras decisiones; (2) que todas estas decisiones serán mas claras y robustas si son producto del consenso entre distintos analistas que trabajan en forma autónoma y que documentan e intercambian la razones de sus decisiones [@Auerbach2003].

En vistas de todas estas decisiones, @Bechmann2019 sugieren:

    Seemingly unsupervised model becomes extremely supervised due to classification work such as setting number of topics, cleaning data in a particular way with an apriori understanding of "meaningful" clusters and interpreting clusters with parent classes manually. 

### Etiquetado manual y organización de los tópicos

El etiquetado no es un proceso distinto al de la codificación cualitativa, es decir, a la interpretación interativa de ideas y expresiones repetidas y la imputación de un código o etiqueta que lo identifica. 

En términos de *codeo*, preparar los datos para esta tarea es muy fácil: simplemente listamos los términos que más contribuyen a cada tópico.

```{r fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
noticias_tm_beta %>% # principales términos en cada tópico
  group_by(topic) %>%
  top_n(15) %>%
  ungroup() %>%
  arrange(topic, -beta) %>% # vamos a mostrarlo como grafico
  ggplot(aes(x=reorder(term, (beta)),y=beta)) + 
    geom_col() +
    facet_wrap(~topic, scales = "free_y") +
  coord_flip()
```

El objetivo del análisis que haremos (manualmente) sobre estos datos es el de evaluar si hay un campo coherente de palabras en cada tópico, para luego asignarle una etiqueta que lo describa. En tanto estas son todas inferencias nuestras, en el mejor de los casos, guidados por nuestro conocimiento teórico del fenómeno, nos ubicamos en el plano de las hipótesis. 

Veamos esto con nuestros datos:

* Tópico 1: sobresalen palabras como tecnología, sistema, mundo, inteligencia y artificial que nos permiten inferir el campo de las transformaciones tecnológicas recientes y sus proyecciones;
* Tópico 2: dato, información, persona, usuario, red (social), nos remiten al proceso que teóricamente llamamos "datificación", al cual entendemos como el trasfondo social del big data [@VanDijck2014];
* Tópico 3: equipo, historia, mundial, selección y jugador nos permiten inferir que se trata del deporte;
* Tópico 4: campañana, gobierno, presidente, política, electoral remiten a elecciones políticas;
* Tópico 5: red, series, sociedad, comunicación, dispositivo, si bien no permiten hipotetizar un tópico coherente, tal vez refieran a plataformas, apps o redes sociales;
* Tópico 6: palabras como empresa, digital, negocio, y cliente, nos permiten inferir el campo semántico del comercio digital.

Vamos a escribir estas 6 etiquetas en un array de nombres de tópicos, por si necesitamos incluirlos en futuros gráficos como etiquetas:

```{r}
topicos_nombres <- c("T1"="1. Transformaciones",
                     "T2"="2. Datificacion",
                     "T3"="3. Deporte",
                     "T4"="4. Elecciones",
                     "T5"="5. Plataformas",
                     "T6"="6. Comercio")
```

Es importante tener en cuenta que no siempre todos los tópicos presentarán un campo semántico coherente: en muchos casos pueden referir a regularidades propias del tipo de comunicación que estamos analizando (e.g., palabras que remiten a una interacción por parte del usuario, si es que estamos trabajando con contenido tomado de páginas interactivas), o una mixtura de palabras tal que el lugar de permitirnos inferir un campo unívoco, nos resulte incoherente.

Luego, debemos organizar nuestros tópicos:

* **¿Descartamos tópicos irrelevantes?**: Más allá de los tópicos incoherentes o para los que un campo semántico no es tan evidente, podemos decidir filtrar otros tópicos en vistas de su (ir)relevancia para nuestra pregunta teórica. En nuestro caso, todos los tópicos (con excepción del #2) parecen ser temas más amplios en los que el big data juega un rol con peso social. El caso del #2 es tal vez distinto, ya que parece ser un tópico que trata focalmente sobre el big data antes que sobre otro fenómeno que lo incluye.

* **¿Agrupar tópicos?**: Generalmente, en una codificación cualitativa, el proceso se repite iterativamente, haciendo inferencias cada vez más generales (mayor abstracción) y coordinadas (mayor coherencia), lo que nos permite pasar de los códigos a los temas y argumentos. El modelo LDA no tiene esa estructura jerárquica, pero nosotros podemos agrupar o colapsar tópicos en temáticas más generales. Esto es casi siempre necesario cuando trabajamos con un K elevado, o con otro algoritmo (como VEM). 

### Análisis cualitativo de documentos

Dados los objetivos de nuestra investigación -indagar las distintas maneras en que se tematiza al big data en la prensa-, no podemos quedarnos con estos resultados que, en el mejor de los casos, son una clasificación probable de los documentos. 
Más bien, nos interesa hacer un *análisis de contenido* manual de algunos documentos que presenten estos tópicos [@Krippendorff2004], no sólo para aclarar las etiquetas dadas, sino también para responder a nuestras preguntas de investigación, teóricamente fundadas. 
(Para el caso de nuestra investigación, se trata de describir cómo se introduce al big data en artículos periodísticos de distintos temas, especialmente, comparando las premisas y las promesas con las que se lo vincula. [@Becerra2019])

En este momento sólo usaremos R para construir la muestra de documentos a analizar manualmente en otro entorno[^1]. 

[^1]: Generalmente para estas tareas se puede utilizar otro software destinado al análisis cualitativo (CAQDAS), como atlas.ti o nvivo. Existe una implementación básica de una interfaz como la de otros CAQDAS en R: RDQA (R package for Qualitative Data Analysis), [https://rqda.r-forge.r-project.org/](https://rqda.r-forge.r-project.org/).

1. etiquetado manual y organización de los tópicos;
2. análisis cualitativo de los documentos;
3. validación estadística y determinación de K;
4. validación de metadatos contra supuestos;
5. explorar relaciones entre tópicos;
6. explorar relaciones y diferencias en los metadatos; 
7. explorar relaciones y diferencias en los contenidos (usando lexicones).











<!-- # ESTABLEZCO UN CRITERIO DE CORTE PARA LAS MUESTRAS DE DOCUMENTOS POR TOPICOS -->

table(topicos_documentos$topic_label) # distribucion en topicos (rank1)
corte_rank1<-mean(topicos_documentos$topic_prob) # valor medio de rank1
corte_rank2<-mean(topicos_documentos$topic_probdiff_2nd) # valor medio de diferencia con rank2
corte <- mean(topicos_documentos$topic_prob) - mean(topicos_documentos$topic_probdiff_2nd) # valor de corte?

x<-topicos_documentos_tidy %>% filter(score>corte_rank2) %>% group_by(topic) %>% tally(sort = TRUE) # %>% summarise(q=mean(n),s=sd(n))
x$topic
x$n
mean(x$n);sd(x$n);sd(x$n)/mean(x$n)

x$n / nrow(noticias_scrap_y_metadata) * 100
  
  

topicos_documentos_graficos <- topicos_documentos_tidy %>% # una tabla auxiliar para simplificar los graficos
  mutate(topic_name = gsub(pattern = "topic_", replacement = "", topic )) %>%
  mutate(topic_id = as.integer(stringi::stri_match_first_regex(topic_name, "(.*?)\\.")[,1]))

topicos_documentos_graficos %>% # distribucion documentos por topicos con cortes
  ggplot(aes(y=score,x=topic_name)) + 
  geom_boxplot() + 
  geom_hline(color="red", yintercept = corte ) + # media de rank1 - diferencia con rank2
  geom_hline(color="blue", yintercept = corte_rank1 ) + # rank1
  geom_hline(color="blue", yintercept = corte_rank2 ) + # rank2
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))+
  scale_y_continuous("topic score", breaks = seq(0,1,0.1))

<!-- # ARMO UNA TABLA CON TOPICOS, TERMINOS, Y SCORES (RANK1 Y CORTE) -->

rank_corte <- as.data.frame(topicos_nombres) %>% # primero armo una tabla con la cantidad de documentos dentro de ese valor de corte ...
  rename(topic_name=topicos_nombres) %>%
  right_join( topicos_documentos_graficos %>% 
    filter(! topic_id %in% topicos_desechados ) %>%
    group_by(topic_name) %>% filter(score > corte) %>% summarize(corte=n()) ) %>% 
  inner_join( # ... y ahora le sumo el rank1 ...
    as.data.frame(table(topicos_documentos$topic_label)) %>% transform(topic_name=as.character(Var1)) %>% 
      rename(Rank1=Freq) %>% select(topic_name,Rank1) 
  )
terminos_rank <- rank_corte %>% transform(topic=as.integer(gsub("\\..*","",topic_name))) %>% inner_join(topicos_palabra_top) # ... y ahora le sumo las palabras ...
write.csv( terminos_rank, file = paste0("terminos_rank_", topicos_K, "topics", "_",format(Sys.time(), "%Y%m%d_%H%M%S"), ".csv")) # ... y lo exporto

<!-- # EXPORTO UNA TABLA DE LINKS A NOTICIAS POR TOPICO (PARA LOS ANALISIS CUALI ES MAS COMODO LEER DESDE LA FUENTE) -->

noticias_para_anotar <- readRDS("noticiasLimpias_2026_20191003_175315.rda")
topicos_documentos %>% 
  select(doc_id, topic, topic_prob, topic_label) %>%
  left_join( noticias_para_anotar %>% select( doc_id, items.link )) %>%
  write.table(file=paste0("noticias_links_topicos_",k_topics,"_topicos.txt") , row.names = FALSE , quote = FALSE , col.names = TRUE , sep = "|")


### Validación estadística y determinación de K

NO SON CONFIABLES, NO SE CORRESPONDEN CON HUMANOS

5 Discussion
We presented the first validation of the assumed coherence and relevance of topic models using human experiments. For three topic models, we demonstrated that traditional metrics do not capture whether topics are coherent or not. Traditional metrics are, indeed, negatively correlated with the measures of topic quality developed in this paper. Our measures enable new forms of model selection and suggest that practitioners developing topic models should thus focus on evaluations that depend on real-world task performance rather than optimizing likelihood-based measures.

Chang, J., Boyd-Graber, J., Gerris, S., Wang, C., & Blei, D. (2009). Reading Tea Leaves: How Humans Interpret Topic Models. Neural Information Processing Systems.

The quality of the model is assessed by computing its perplexity, i.e. a metric based on the probability of the documents held out for evaluation. Hyperparameter settings then can also be optimized according to highest held out likelihood (Wallach et al., 2009). Although likelihood evaluation is widely used due to its pure automatic nature, Chang et al. (2009) have proven with large user studies that optimal held out likelihood does not correspond to human perception of semantic coherence of topics.




comparing measurements of semantic coherence of topics (see eq. 3.15) as an additional hint to identify overly broad or incoherent topics, and


### Comparar tópicos


<!-- # CALCULO DISTANCIA ENTRE LOS VOCABULARIOS DE CADA TOPICO -->

topicos_palabra_phi <- as.data.frame(t( topicos_palabra_tidy %>% spread(key = topic, value = beta) )) # en lo que sigue preparo phi, para calcular Hellinger
colnames(topicos_palabra_phi) <- as.vector( t( topicos_palabra_phi[1,] ))
topicos_palabra_phi <- topicos_palabra_phi[-1,] 
rownames(topicos_palabra_phi[-topicos_desechados,]) <- topicos_nombres[-topicos_desechados]
topicos_palabra_phi <- as.matrix(topicos_palabra_phi)
topicos_palabra_phi <- type.convert(topicos_palabra_phi)
hc_vocabulario<-hclust(as.dist(textmineR::CalcHellingerDist(topicos_palabra_phi[-topicos_desechados,])), "ward.D")
plot(hc_vocabulario,labels = topicos_nombres[-topicos_desechados], sub = "", main = "" , xlab = "Topics")
rm(topicos_palabra_phi,hc_vocabulario)


<!-- # CORRELACIONES DE TOPICOS (por distribucion basica en documentos) -->

lqs<-setdiff((1:25+5),(topicos_desechados+5))
topic_scores_corr <-  cor( topicos_documentos[lqs] , method = "pearson") # correlacion entre topicos asignados a articulos
ggcorrplot::ggcorrplot(topic_scores_corr, outline.col = "white", show.legend = FALSE,
                       type = "lower", colors = colorRampPalette(c("red","white","green"))(3),
                       lab = TRUE, insig = "blank", method = "square", 
                       hc.order = TRUE , hc.method = "complete")
ggsave(filename = "graficos/correlaciones.jpg",width=15,height = 15,units = "cm",dpi=300)
topic_scores_corr %>% corrr::correlate() %>% corrr::network_plot()
rm(lqs, topic_scores_corr)



### Buscar variaciones en metadatos ()

fecha: hay hechos relevantes?

distribución por source


```{r}
noticias_tm_gamma %>%
  mutate(id=as.integer(document)) %>%
  left_join(noticias %>% select(id,fuente)) %>%
  ggplot(aes(x=fuente,y=gamma)) +
    geom_boxplot() +
    facet_grid(~topic) + 
    coord_flip()
```



### Buscar variaciones en contenidos (lexicones)


<!-- # % articulos que dicen "big data" en los primeros 200 x topico -->

str(topicos_documentos_graficos)
topicos_documentos_graficos %>% # proporcion de articulos que dicen big data en titulo
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score) %>%
  left_join( noticias_para_anotar %>% select( doc_id, keyTitulo ) ) %>%
  group_by(topic_name) %>%
  summarise(bgEnTitulo=sum(keyTitulo)) %>%
  mutate( bgEnTitulo = (bgEnTitulo/200)*100 ) %>%
  ggplot(aes(x=reorder(topic_name,bgEnTitulo),y=bgEnTitulo)) + 
  geom_col() +
  theme_minimal() + 
  coord_flip() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/bg_en_titulo.jpg",width=6,height = 7,units = "cm",dpi=300)

<!-- # q articulos que dicen las 3 v -->

x<-topicos_documentos_graficos %>% # proporcion de articulos que dicen las 3 v
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score) %>%
  left_join( noticias_para_anotar %>% select( doc_id, text ) ) %>%
  mutate( 
    volumen = if_else( grepl("volumen", text, ignore.case = TRUE) ,  1 , 0),
    variedad = if_else( grepl("variedad", text, ignore.case = TRUE), 1 , 0),
    velocidad = if_else( grepl("velocidad", text, ignore.case = TRUE), 1 , 0) ) 
x <- x %>% mutate( vvv = volumen + variedad + velocidad)  
x %>% group_by( topic_name ) %>% summarise(n=sum(vvv)) %>%
  ggplot(aes(x=reorder(topic_name,n),y=n)) + 
  geom_col() +
  theme_minimal() + 
  coord_flip() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/3v.jpg",width=6,height = 7,units = "cm",dpi=300)
rm(x)

<!-- # q articulos que dicen *byte -->

topicos_documentos_graficos %>% # proporcion de articulos que dicen las 3 v
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score) %>%
  left_join( noticias_para_anotar %>% select( doc_id, text ) ) %>%
  mutate( 
    byte = if_else( grepl("byte", text, ignore.case = TRUE) ,  1 , 0)) %>% 
  group_by( topic_name ) %>% summarise(n=sum(byte)) %>%
  ggplot(aes(x=reorder(topic_name,n),y=n)) + 
  geom_col() +
  theme_minimal() + 
  coord_flip() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/byte.jpg",width=6,height = 7,units = "cm",dpi=300)

<!-- # TOPICO 16, 1 Y 17: BIG DATA -->

<!-- # q articulos que dicen platform -->

x<-topicos_documentos_graficos %>% # proporcion de articulos que dicen las 3 v
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score) %>%
  left_join( noticias_para_anotar %>% select( doc_id, text ) ) %>%
  mutate( 
    plataforma = if_else( grepl("plataforma", text, ignore.case = TRUE) ,  1 , 0),
    algoritmo = if_else( grepl("algoritmo", text, ignore.case = TRUE), 1 , 0),
    colec = if_else( grepl("colec", text, ignore.case = TRUE), 1 , 0) ) 
x %>% group_by( topic_name ) %>% 
  #summarise(n=sum(plataforma)) %>%
  summarise(n=sum(algoritmo)) %>%
  #summarise(n=sum(colec)) %>%
  ggplot(aes(x=reorder(topic_name,n),y=n)) + 
  geom_col() +
  theme_minimal() + 
  coord_flip() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/plataforma.jpg",width=6,height = 7,units = "cm",dpi=300)
ggsave(filename = "graficos/algoritmo.jpg",width=6,height = 7,units = "cm",dpi=300)
rm(x)

<!-- # TOPICO 10 y 15 : BIG DATA EN ELECCIONES -->

x<-topicos_documentos_graficos %>% # proporcion de articulos que dicen las 3 v
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score)
x<-x %>% left_join( noticias_para_anotar %>% select( doc_id, text ) ) %>%
  mutate( 
    Macri = if_else( grepl("macri", text, ignore.case = TRUE) ,  1 , 0),
    Kirchner = if_else( grepl("kirchner", text, ignore.case = TRUE), 1 , 0),
    Trump = if_else( grepl("trump", text, ignore.case = TRUE), 1 , 0),
    Clinton = if_else( grepl("clinton", text, ignore.case = TRUE), 1 , 0),
    Nix = if_else( grepl("nix", text, ignore.case = TRUE), 1 , 0),
    Zuckerberg = if_else( grepl("zuckerberg", text, ignore.case = TRUE), 1 , 0)
  )
x <- x %>% group_by( topic_name ) %>% select(-(1:6)) %>% summarise_all(funs(sum)) %>% gather(key="actor",value="n",-topic_name)
x
x %>% ggplot(aes(x=reorder(topic_name,n),y=n,fill=actor)) + 
  geom_col()+
  theme_minimal() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.title = element_blank())+
  coord_flip()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/politicos.jpg",width=12,height = 8,units = "cm",dpi=300)
x %>% ggplot(aes(x=reorder(topic_name,n),y=n,fill=actor)) + 
  geom_bar(position="fill", stat="identity")+
  theme_minimal() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.title = element_blank())+
  coord_flip()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/politicos2.jpg",width=12,height = 8,units = "cm",dpi=300)

<!-- # MECKOCHART DE PERSONALIDADES -->

x2<-x%>%inner_join(x %>% group_by(doc_id,topic_name) %>% select(-(1:6)) %>% summarize(Actores=sum(Macri,Kirchner,Trump,Clinton,Nix,Zuckerberg)))
x2
x2 <- x %>% inner_join(menciones)
x2
menciones<-x %>% group_by(topic_name)%>%summarize(Menciones=sum(Actores))
menciones$xmax<-cumsum(menciones$Menciones)
menciones$xmin<-menciones$xmax - menciones$Menciones
menciones
rm(x2)
rm(menciones)
rm(x)

<!-- # TOPICO 18, 20, 22, 23 : BIG DATA EN BIZ -->

x<-topicos_documentos_graficos %>% # proporcion de articulos que dicen las 3 v
  filter(! topic_id %in% topicos_desechados ) %>%
  group_by(topic) %>%
  top_n(200,wt = score) %>%
  left_join( noticias_para_anotar %>% select( doc_id, text ) ) %>%
  mutate( 
    revolucion = if_else( grepl("revolucion", text, ignore.case = TRUE) ,  1 , 0),
    cuarto = if_else( grepl("cuarto", text, ignore.case = TRUE), 1 , 0),
    digital = if_else( grepl("digital", text, ignore.case = TRUE), 1 , 0) ) 

x %>% group_by( topic_name ) %>% summarise(n=sum(revolucion)) %>%
  ggplot(aes(x=reorder(topic_name,n),y=n)) + 
  geom_col() +
  theme_minimal() + 
  coord_flip() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank() ) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank()) 
ggsave(filename = "graficos/revolucion.jpg",width=6,height = 7,units = "cm",dpi=300)
rm(x)
