# Entrenamiento no-supervisado: Modelamiento de tópicos

En este capítulo vamos a intentar explorar **¿Cuál es la tematización del big data en la prensa?**, utilizando un corpus de noticias que incluyen la palabra "big data", tomadas de periódicos digitales argentinos. 

Particularmente nos interesa una técnica particular dentro del campo del procesamiento del lenguaje natural (NLP), que busca construir tópicos en base a las distribuciones de palabras en un conjunto de documentos, generalmente conocida como *topic modeling*. 

A lo largo de este ejercicio veremos:

1. cómo pre-procesar texto para su posterior análisis;
2. cómo construir tablas que crucen documento x términos;
2. cómo modelar tópicos;
3. cómo interpretar los tópicos.

### Cargar datos y recursos

En lo que sigue vamos a trabajar mayormente con funciones del paquete `tidyverse` para XXX.

<!-- 2do: librerias -->

<!-- 2do: definir fuente de datos -->
<!-- 2do: meter un id -->

```{r echo=TRUE, message=TRUE, warning=FALSE}
library(tidyverse) 

noticias <- readRDS(file = "data/noticias_curso.rds") %>% mutate(id=1:n())
glimpse(noticias) # miramos la estructura de la base
```

### Pre-procesamiento de texto

Para poder completar nuestros análisis primeros realizaremos varias tareas de preprocesamiento:

1. Haremos un análisis gramatical para determinar los distintos componentes de la oración;
2. Reduciremos las palabras a sus *lemmas*, formas básicas de las palabras, sin género ni conjugación;
3. Descartaremos algunas palabras comunes, quedándonos sólo con las más significativas.

Para estas tareas trabajaremos con la librería UdPipe, desarrollada por el [Instituto de linguistica formal y aplicada de la Universidad de la República Checa](https://ufal.mff.cuni.cz/udpipe), que tiene un modelo para procesar texto en castellano.

Lo primero que debemos hacer es instalar la librería. Luego, deberemos descargar el modelo del idioma que nos interesa. 

```{r, eval=F, echo=T}
install.packages("udpipe")
modelo_sp <- udpipe::udpipe_download_model('spanish') # descarga el modelo y guarda la referencia en un objeto modelo_sp$file_model
```

O si ya lo tenemos descargado, es conveniente referenciarlo:

```{r, eval=T, echo=T}
library(udpipe)
modelo_sp <- udpipe_load_model(file = "../dix/spanish-gsd-ud-2.5-191206.udpipe")
```

Con el modelo ya estamos en condiciones de empezar a *parsear* nuestro corpus de oraciones, y *anotar* qué tipo de componente es cada palabra, además de lemmatizarlas.

```{r eval=F, echo=T}

noticias_anotadas <- udpipe_annotate( 
  object = modelo_sp, # el modelo de idioma
  x = noticias$txt, # el texto a anotar, 
  doc_id = noticias$id, # el id de cada oracion (el resultado tendrá 1 palabra x fila)
  trace = 50
  ) %>% as.data.frame(.) # convertimos el resultado en data frame

```

```{r message=FALSE, warning=FALSE, include=FALSE}

# guardamos el parseado
# readr::write_csv(x = noticias_anotadas, file = "data_ignore/noticias_anotadas.csv")

# tomamos el parseado
noticias_anotadas <- readr::read_csv(file = "data_ignore/noticias_anotadas.csv")

```

Vamos a examinar la tabla con las oraciones parseadas:

```{r}
glimpse(noticias_anotadas)
```

Las siguientes variables nos interesan para preparar los datos para el análisis:

* `doc_id`: dado que la tabla es "larga", y tiene 1 fila por cada palabra de cada oración, el doc_id nos permitirá volver a unir las piezas cuando hagamos tareas por oraciones; 
* `lemma`: incluye la forma lemmatizada de la palabra, sin género, en singular, y sin conjugación, y la vuelve minúscula (e.g., "es" se vuelve "ser", y "Datos" se conviernete en "dato");
* `upos`: nos permite aclarar qué tipo de palabra es;

También notemos que la anotación convirtió la columna `doc_id` en una columna de texto. Conviene que corrijamos esto:

```{r}
noticias_anotadas <- noticias_anotadas %>% mutate(doc_id=as.integer(doc_id))
```


<!-- 2do: con deps o upos se puede centrar en oraciones sobre big data? -->

Aquí empiezan las decisiones que afectarán la calidad de los análisis, en relación al tipo de resultados que busquemos.

Nuestra primera decisión es aprovechar la información de `upos` para quedaremos con las palabras que probablemente tengan mayor peso para conferir una orientación positiva o negativa: adjetivos, verbos, sustantivos y adverbios.

Además, filtraremos la palabra "bigdata" ya que es esta justamente la que queremos definir por su contexto.

(También seleccionaremos algunas columnas para trabajar, pero esto solamente por comodidad).

```{r}
noticias_anotadas2 <- noticias_anotadas %>% 
  filter(upos=="ADJ"| upos=="VERB"| upos=="NOUN" | upos=="ADV") %>% # filtramos por tipo de palabra
  select( doc_id, lemma ) # seleccionamos solo las columnas que nos interesan, esto no es necesario
  
glimpse(noticias_anotadas2)
```

Es interesante señalar que pasamos de `r nrow(noticias_anotadas)` palabras parseadas y anotadas, a solo `r nrow(noticias_anotadas2)`, que son con las que trabajaremos.

### Vectorizar

xxx

